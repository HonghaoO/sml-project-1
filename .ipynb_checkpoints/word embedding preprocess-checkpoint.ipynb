{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad64274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703e59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the data sets\n",
    "\n",
    "set1_human = pd.read_json('dataset/set1_human.json')\n",
    "set1_machine = pd.read_json('dataset/set1_machine.json')\n",
    "set2_human = pd.read_json('dataset/set2_human.json')\n",
    "set2_machine = pd.read_json('dataset/set2_machine.json')\n",
    "set_test = pd.read_json('dataset/test.json')\n",
    "\n",
    "# Add in label for all data sets\n",
    "set1_human = set1_human.assign(label=1)\n",
    "set2_human = set2_human.assign(label=1)\n",
    "set1_machine = set1_machine.assign(label=0)\n",
    "set2_machine = set2_machine.assign(label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2169a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate and concatenate into 2 sets, set 1 and set 2\n",
    "set1 = pd.concat([set1_human[:3500], set1_machine])\n",
    "set2 = pd.concat([set2_human, set2_machine[:100]])\n",
    "\n",
    "# Split the labels and the data for training\n",
    "x_set1 = pd.DataFrame(set1[['txt', 'prompt']])\n",
    "y_set1 = pd.DataFrame(set1[['label']])\n",
    "x_set2 = pd.DataFrame(set2[['txt', 'prompt']])\n",
    "y_set2 = pd.DataFrame(set2[['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d55e486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_set1['prompt_len'] = x_set1['prompt'].apply(lambda x: len(x))\n",
    "x_set1['txt_len'] = x_set1['txt'].apply(lambda x: len(x))\n",
    "\n",
    "x_set2['prompt_len'] = x_set2['prompt'].apply(lambda x: len(x))\n",
    "x_set2['txt_len'] = x_set2['txt'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d1e62a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_combined_set = pd.concat([x_set1, x_set2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "62b9715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_combined_set = pd.concat([y_set1, y_set2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e39066fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "def word_embedding(x_combined_set):\n",
    "    vocab = x_combined_set['txt'].tolist() + x_combined_set['prompt'].tolist()\n",
    "    model = Word2Vec(sentences=vocab, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    model.save(\"word2vec.model\")\n",
    "    \n",
    "    w2v_model =  gensim.models.Word2Vec.load(\"word2vec.model\").wv\n",
    "\n",
    "    x_prompt = x_combined_set['prompt']\n",
    "\n",
    "    x_prompt_vec = []\n",
    "    for sentence in x_prompt:\n",
    "        sentence_vec = []\n",
    "    \n",
    "        for word in sentence:\n",
    "            sentence_vec.append(w2v_model[word])\n",
    "        x_prompt_vec.append(np.mean(sentence_vec, axis=0))\n",
    "    x_prompt_vec = np.array(x_prompt_vec)\n",
    "    \n",
    "    x_text = x_combined_set['txt']\n",
    "\n",
    "    x_text_vec = []\n",
    "    for sentence in x_text:\n",
    "        sentence_vec = []\n",
    "    \n",
    "        for word in sentence[:256]:\n",
    "            sentence_vec.append(w2v_model[word])\n",
    "        x_text_vec.append(np.mean(sentence_vec, axis=0))\n",
    "    x_text_vec = np.array(x_text_vec)\n",
    "    \n",
    "    prompt_len = np.expand_dims(x_combined_set['prompt_len'].values , axis=1)\n",
    "    txt_len = np.expand_dims(x_combined_set['txt_len'].values , axis=1)\n",
    "    \n",
    "    x_set = np.concatenate((x_text_vec, x_prompt_vec, prompt_len, txt_len), axis = 1)\n",
    "    return x_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9d940fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaming/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       367\n",
      "           1       0.88      0.92      0.90       333\n",
      "\n",
      "    accuracy                           0.90       700\n",
      "   macro avg       0.90      0.90      0.90       700\n",
      "weighted avg       0.90      0.90      0.90       700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaming/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "x_set1 = word_embedding(x_set1)\n",
    "x_train1, x_test1, y_train1, y_test1= train_test_split(x_set1, y_set1, test_size=0.1, stratify=y_set1)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train1, y_train1)\n",
    "y_pred = clf.predict(x_test1)\n",
    "\n",
    "print(classification_report(y_pred, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "32ff1081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaming/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/jiaming/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.47      0.43       295\n",
      "           1       0.55      0.48      0.51       405\n",
      "\n",
      "    accuracy                           0.47       700\n",
      "   macro avg       0.47      0.47      0.47       700\n",
      "weighted avg       0.49      0.47      0.48       700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "def lgbcls(x_train, y_train):\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 10,\n",
    "        'learning_rate': 0.001,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    lgb_clf = lgb.LGBMClassifier(**params)\n",
    "    lgb_clf.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return y_pred\n",
    "\n",
    "y_pred = lgbcls(x_train1, y_train1)\n",
    "print(classification_report(y_pred, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4c538792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.61      0.58        18\n",
      "           1       0.65      0.59      0.62        22\n",
      "\n",
      "    accuracy                           0.60        40\n",
      "   macro avg       0.60      0.60      0.60        40\n",
      "weighted avg       0.61      0.60      0.60        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaming/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "x_set2 = word_embedding(x_set2)\n",
    "x_train2, x_test2, y_train2, y_test2= train_test_split(x_set2, y_set2, test_size=0.2, stratify=y_set2)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train2, y_train2)\n",
    "y_pred = clf.predict(x_test2)\n",
    "accuracy = accuracy_score(y_test2, y_pred)\n",
    "print(classification_report(y_pred, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7b40c87e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiaming/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jiaming/.local/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_237/3338010201.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_prompt_vec = np.array(x_prompt_vec)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [144]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m x_set0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x_set0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m      4\u001b[0m x_set0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x_set0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m----> 5\u001b[0m new_x_set \u001b[38;5;241m=\u001b[39m \u001b[43mword_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_set0\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [140]\u001b[0m, in \u001b[0;36mword_embedding\u001b[0;34m(x_combined_set)\u001b[0m\n\u001b[1;32m     33\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(x_combined_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues , axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m txt_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(x_combined_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues , axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m x_set \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_text_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_prompt_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_set\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "set0 = pd.concat([set1, set_test])\n",
    "x_set0 = pd.DataFrame(set0[['txt', 'prompt']])\n",
    "x_set0['prompt_len'] = x_set0['prompt'].apply(lambda x: len(x))\n",
    "x_set0['txt_len'] = x_set0['txt'].apply(lambda x: len(x))\n",
    "new_x_set = word_embedding(x_set0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d5955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_set_train = new_x_set[:7000]\n",
    "x_set_test = new_x_set[7000:]\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_set_train, y_set1)\n",
    "y_pred = classifier.predict(x_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09533034",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set_test['txt'].tolist() + set_test['prompt'].tolist()\n",
    "model = Word2Vec(sentences=vocab, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "    \n",
    "w2v_model =  gensim.models.Word2Vec.load(\"word2vec.model\").wv\n",
    "\n",
    "\n",
    "test_text = set_test['txt']\n",
    "\n",
    "test_vec = []\n",
    "for sentence in test_text:\n",
    "    sentence_vec = []\n",
    "    \n",
    "    for word in sentence[:256]:\n",
    "        sentence_vec.append(w2v_model[word])\n",
    "    test_vec.append(np.mean(sentence_vec, axis=0))\n",
    "test_vec = np.array(test_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
